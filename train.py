# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

"""

from google.colab import drive
drive.mount('/content/drive/')

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout

# Set random seeds to ensure the reproducible results
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

#  Please edit these directory before running the code.
# dataset for training model
data_dir = '/content/drive/My Drive/app/traindata_with_extra_images/'
# the directory to store model
model_save_dir = '/content/drive/My Drive/app/model/'


def load_data():
    # use data augmentation to create more instance to train and validate the model
    # use split parameter to spilt the data into the training and validation
    batch_size = 32
    # Our original images consist in RGB coefficients in the 0-255, but such values
    # would be too high for our model to process (given a typical learning rate),
    # so we target values between 0 and 1 instead by scaling with a 1/255.
    train_datagen = ImageDataGenerator(validation_split=0.2,
                                       rescale=1. / 255,
                                       rotation_range=30,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True)

    # Normalize the test data images, resize them but don't augment them, to eliminate the
    # uncertainty of testing results.
    validation_datagen = ImageDataGenerator(rescale=1. / 255, validation_split=0.2)

    # load the data from the training directory and set them to two flows training /validation
    train = train_datagen.flow_from_directory(data_dir, target_size=(300, 300), shuffle=True,
                                              classes=['cherry', 'strawberry', 'tomato'],
                                              batch_size=batch_size, subset='training')

    validation = validation_datagen.flow_from_directory(data_dir, target_size=(300, 300), shuffle=True,
                                                        batch_size=batch_size, subset='validation')
    return train, validation


def build_model(flag):
    if flag == 'MLP':
        model = Sequential()
        # layer 1
        model.add(Flatten(input_shape=(300, 300, 3)))
        model.add(Dense(256, activation='relu'))
        # layer 2
        model.add(Dense(128, activation='relu'))
        # layer 3
        model.add(Dense(3, activation='softmax'))

        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    if flag == 'optimize_CNN':
        #  apply regulizer L2 = 0.001 to the three layers with the highest parameters.
        #  apply dropout only on the layer with many parameters.
        #  final model comes from model_5
        model = Sequential()
        # layer 1
        model.add(Conv2D(32, (3, 3), input_shape=(300, 300, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        # layer 2
        model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
        # layer 3
        model.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
        # layer 4
        model.add(Flatten())
        model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
        model.add(Dropout(0.25))
        # layer 5
        model.add(Dense(3, activation='softmax'))

        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model


def transfer_learning_model():
    # Create the base model from the pre-trained model MobileNet V2
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=(300, 300, 3),
        include_top=False,
        weights='imagenet')

    # freeze the convolutional base
    base_model.trainable = False

    # wrap the model in a sequential model
    model = Sequential()
    model.add(base_model)
    model.add(tf.keras.layers.GlobalAveragePooling2D())
    # We don't need Flatten layer here as GlobalAveragePooling2D did so
    model.add(Dropout(0.25))
    model.add(Dense(3, activation='softmax'))

    # train the trail of the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(train, validation_data=validation, epochs=15)
    print("Number of layers in the base model: ", len(base_model.layers))

    # Number of layers in the base model:  154
    # Unfreeze some of the pre-trained model and continue training
    base_model.trainable = True
    for layer in base_model.layers[:100]:
        layer.trainable = False

    # reduce learning_rate to prevent overfitting
    model.compile(optimizer=RMSprop(learning_rate=1e-5),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    
    print("First fine-tune starts:")
    model.fit(train, validation_data=validation, initial_epoch=15, epochs=30)

    # Unfreeze some of the pre-trained model and continue training
    base_model.trainable = True
    for layer in base_model.layers[:50]:
        layer.trainable = False

    # further reduce learning_rate to prevent overfitting
    model.compile(optimizer=RMSprop(learning_rate=3e-6),
                  loss='categorical_crossentropy', metrics=['accuracy'])

    earlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')
    checkPoint = ModelCheckpoint(filepath=model_save_dir + 'model.h5', monitor='val_loss', verbose=1,
                                 save_best_only=True, mode='min')
    
    print("Second fine-tune starts:")
    model.fit(train, validation_data=validation, initial_epoch=30, epochs=40, callbacks=[earlyStop, checkPoint])
    return model


def train_model(model, train, validation):
    earlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')
    checkPoint = ModelCheckpoint(filepath=model_save_dir + 'model.h5', monitor='val_loss', verbose=1,
                                 save_best_only=True, mode='min')
    history = model.fit(train, validation_data=validation, epochs=100, verbose=1, callbacks=[earlyStop, checkPoint])
    return history, model


if __name__ == '__main__':
    train, validation = load_data()
    # For the CNN model tuned by me
    #model = build_model('optimize_CNN')
    #history, model = train_model(model, train, validation)

    # For transfer learning model
    model = transfer_learning_model()
